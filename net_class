Agent学习

（接受输入，思考，输出）+记忆+工具+规划
prompt （少样本提示、思维链、思维树...）
主要是不光要写好prompt，还要写好策略。

tools (自己使用)

memory （自己管理，进度）

plan （自己干预）

一个人的视角
一个项目经理的视角

感知 规划 行动 观察

Agnet的目的，为了处理那些简单的语言模型可能无法直接解决的问题，当任务涉及到多个步骤或者需要外部数据源的情况。

需要思考任务何时可以终止。


#将DataFrame转换为JSON格式,orient='split'参数将数据、索引和列分开存储
df_complex_json = df_complex.to_json(orient='split')


--------------------------------------------------------------------------------------------------

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)


tools = [
    Tool(
        name = "Search",
        func=search.run,
        description="useful for when you need to answer questions about current events or the current state of the world"
    ),
    Tool(
        name="Calculator",
        func=llm_math_chain.run,
        description="useful for when you need to answer questions about math"
    ),
]



agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True,
    handle_parsing_errors=True,#处理解析错误
    agent_kwargs={
        "extra_prompt_messages":[MessagesPlaceholder(variable_name="chat_history"),MessagesPlaceholder(variable_name="agent_scratchpad")],
    },
    memory=memory #记忆组件
    )

====================================================================================================


Function Calling 赋予大语言模型调用外部API的能力。
书写规则：
1、明确的函数名：选择一个清晰、描述性强的函数名。
2、参数顺序和命名：参数应有逻辑顺序，并使用描述性强的名称。
3、详细的函数描述：对函数功能和调协的参数变量有明确的说明。

autodl注意
1、sdd空间
2、cuda 版本
3、jupyter book
4、隧道

操作：

本文的基础环境如下
操作系统：ubuntu22.04
CUDA：12.4
英伟达显卡驱动版本：550.78
显卡型号：本实验用英伟达3090（全模型（未开量化）需要单张显存大于20GB的英伟达显卡；）


cd ~/ && wget https://file.huishiwei.top/LLaMA-Factory.tar.gz
tar -xvf LLaMA-Factory.tar.gz

# 创建虚拟环境（如果已创建，请忽略此步骤）
conda create -n llama_factory -y python=3.11 pip
# 激活虚拟环境
conda activate llama_factory
如果报错：需要初始化conda  用conda init bash

# 使用pip安装依赖
cd ~/LLaMA-Factory
pip install -e ".[torch,metrics]" #安装本地的包，不用在互联网上去下载安装
pip install modelscope -U



#requirements_llm.txt
pip freeze > requirements.txt

cat <<EOL > ~/LLaMA-Factory/requirements_llm.txt
accelerate==0.34.2
aiofiles==23.2.1
aiohappyeyeballs==2.4.0
aiohttp==3.10.5
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.4.0
attrs==24.2.0
bitsandbytes==0.43.3
certifi==2024.8.30
charset-normalizer==3.3.2
click==8.1.7
contourpy==1.3.0
cycler==0.12.1
datasets==2.21.0
dill==0.3.8
docstring_parser==0.16
einops==0.8.0
fastapi==0.114.1
ffmpy==0.4.0
filelock==3.16.0
fire==0.6.0
fonttools==4.53.1
frozenlist==1.4.1
fsspec==2024.6.1
gradio==4.44.0
gradio_client==1.3.0
h11==0.14.0
httpcore==1.0.5
httpx==0.27.2
huggingface-hub==0.24.6
idna==3.8
importlib_resources==6.4.5
jieba==0.42.1
Jinja2==3.1.4
joblib==1.4.2
kiwisolver==1.4.7
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.9.2
mdurl==0.1.2
modelscope==1.19.0
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
networkx==3.3
nltk==3.9.1
numpy==1.26.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.6.68
nvidia-nvtx-cu12==12.1.105
orjson==3.10.7
packaging==24.1
pandas==2.2.2
peft==0.12.0
pillow==10.4.0
protobuf==5.28.0
psutil==6.0.0
pyarrow==17.0.0
pydantic==2.9.1
pydantic_core==2.23.3
pydub==0.25.1
Pygments==2.18.0
pyparsing==3.1.4
python-dateutil==2.9.0.post0
python-multipart==0.0.9
pytz==2024.2
PyYAML==6.0.2
regex==2024.7.24
requests==2.32.3
rich==13.8.1
rouge-chinese==1.0.3
ruff==0.6.4
safetensors==0.4.5
scipy==1.14.1
semantic-version==2.10.0
sentencepiece==0.2.0
shellingham==1.5.4
shtab==1.7.1
six==1.16.0
sniffio==1.3.1
sse-starlette==2.1.3
starlette==0.38.5
sympy==1.13.2
termcolor==2.4.0
tiktoken==0.7.0
tokenizers==0.19.1
tomlkit==0.12.0
torch==2.4.1
tqdm==4.66.5
transformers==4.44.2
triton==3.0.0
trl==0.9.6
typer==0.12.5
typing_extensions==4.12.2
tyro==0.8.10
tzdata==2024.1
urllib3==2.2.2
uvicorn==0.30.6
websockets==12.0
xxhash==3.5.0
yarl==1.11.1


# 查看LLaMA-Factory版本
llamafactory-cli version
# 如果出现下面信息，代表安装成功
----------------------------------------------------------
| Welcome to LLaMA Factory, version 0.9.1.dev0           |
|                                                        |
| Project page: https://github.com/hiyouga/LLaMA-Factory |
----------------------------------------------------------


#启动web界面
export USE_MODELSCOPE_HUB=1 # 使用modelscope下载模型
export NCCL_P2P_DISABLE="1"
export NCCL_IB_DISABLE="1"
export MODELSCOPE_CACHE='/root/autodl-tmp/modelscope/'
export MODELSCOPE_MODULES_CACHE='/root/autodl-tmp/modelscope/modelscope_modules'
llamafactory-cli webui


#设置隧道
ssh -CNgv -L 7860:127.0.0.1:7860 root@connect.cqa1.seetacloud.com -p 51363

设置隧道后，应该可以通过本机地址：http://127.0.0.1:7860打开llamafactory的ui界面，通过在Lang选项卡中，选择zh即可将界面调整为中文。

GLM-4-9B-Chat

watch nvidia-smi



# 创建自定义数据集
./LLaMAFactory/data/,创建一个名为my_demo.json的文件
[
    {
        "instruction": "你好",
        "input": "",
        "output": "您好，我是信息中心AI助手，一个由信息中心AI研发中心 开发的AI助手，很高兴 认识你"
    },
    ...
]


#更新dataset_info.json文件
~/LLaMA-Factory/data/dataset_info.json文件：
"my_demo": {
    "file_name": "my_demo.json"
},

开始tune
1、模型名称选择模型
2、数据集那儿选数据集
3、学习率那儿调整学习率


学习率 5e-4 调大 从-5到-4
训练轮数增加或减少



测试微调效果：
1、选择模型
2、选择检查点
3、加载模型





改源码问题：
vscode 装 remote-ssh(ms)的plug-inc  #可以改远程上的文件














修复一个bug ，打开/home/llm_course/chinese-LLaMA-Alpaca-3-3.0/scripts/oai_api_demo/openai_api_server.py文件，
def stream_pedict(中 增加两行
pad_token_id=tokenizer.eos_token_id,
eos_token_id=[tokenizer.eos_token_id,tokenizer.convert_tokens_to_ids("<|eot_id|>")]











