以下的langchain的版本为0.38

六大模块：
模型I/O
检索
Agent
Chains
Memory
Callbacks

1、安装
pip install langchain-core langchain-community langchain-openai
pip install langchain-ollama


3.1 LangChain的模块
3.1.1 模块
Model I/O 模块(语言模型 指令 输出解析器)
检索模块（向量数据及外部数据）
内存
链
代理（Agent）
回调



3.2
3.2.1 LangSmith
3.2.2 调试LLM应用
langchain.debug=True
langchain.verbose=True

3.3 langchain回退
有效的容错策略，旨在确保调用API失败或性能下降时，能维护正常的运行状态。
3.3.1 处理LLM调用API的错误
llmOpenAI=ChatOpenAI()
llmOllama=ChatOllama()
llm=llmOpenAI.with_fallbacks([llmOllama])

3.3.2
AsyncCallbacks.py 重点复习



4.1.6 缓存
1）内存缓存
from langchain_community.cache import InMemoryCache
from langchain.globals import set_llm_cache

langchain.llm_cache = InMemoryCache()
#set_llm_cache(InMemoryCache())
llm.invoke("first test cache")
llm.invoke("first test cache")

2)SQLite缓存
from langchain.cache import SQLiteCache
langchain.llm_cache=SQLiteCache(database_path=".langchain.db")
llm.invoke("first test cache")
llm.invoke("first test cache")

3)序列化LLM配置
llm.save("llm.json")
llm.save("llm.yaml")
from langchain_community.llms.loading import load_llm
load_llm(filename)

4)llm比较
from langchain.model_laboratory import ModelLaboratory
model_lab=ModelLaboratory.from_llms(llms)
model_lab.compare(message)


4.4 解析器
4.4.1 列表解析器
from langchain_core.output_parsers import CommaSeparatedListOutputParser
output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="List five {subject}.\n{format_instructions}",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions}
)
...
output_parser.parse(response.content)

4.4.2 日期时间解析器
from langchain.output_parsers import DatetimeOutputParser
prompt = PromptTemplate.from_template(template, partial_variables={"format_instructions": output_parser.get_format_instructions()})
prompt = PromptTemplate.from_template(template, partial_variables={
chain=prompt | llm

4.4.3 Json解析器


5.编写Prompt的策略
5.1 使用明确且具体的指令
5.2 添加明确的语法
  比如用：作为分隔符，对问题更好进行说明。
5.3 最后重复一遍指令
5.4 引导输出操作
5.5 引导输入格式
  比如： 回答“是的”或“不是”
5.6 细化任务的分解
5.7 思维链指令
5.8 提供真实的上下文 
5.9 限定结构化输出
5.10 指定完成任务所需的步骤
5.11 设定角色与人物

6.Prompt模板
1）字符串模板
PromptTemplate
PromptTemplate.from_template()

2）聊天指令模板
每条消息均包含内容，且附加role参数
ChatPromptTemplate
ChatPromptTemplate.from_message()

AIMessagePromptTemplate
SystemMessagePromptTemplate
HumanMessagePromptTemplate

ChatMessagePromptTemplate
1


7.数据
TextLoader
CSVLoader
DirectoryLoader
  use_multithreading=True
  loader_cls=[TextLoader|PythonLoader]
  silent_errors=True #skip error file
    text_load_kwargs={'autodetect_encoding': True} #文件编码自动检测
  loader_kwargs=text_load-kwargs
UnstructuredHTMLLoader

UnstructuredPDFLoader
  pdfminer要装pdfminer.six这个版本
  pi_heif
  unstructured_inference
  pdf2image

拆分
RecursiveCharacterTextSplitter
  length_function 计算长度的函数
  chunk_size 每个块的大小
  chunk_overlap 重叠块大小
  add_start_index 块中包含起始位置
  .from_language(lnaguage=Language.PYTHON,)

CharacterTextSplitter
  separator="\n\n"
  chunk_size 每个块的大小
  chunk_overlap 重叠块大小
  add_start_index 块中包含起始位置
  is_separator_regex

  length_function 计算长度的函数

8.2使用redis缓存向量
store = RedisStore(redis_url="redis://192.168.0.222:6379", client_kwargs={'db': 2}, namespace='embedding_caches')
underlying_embeddings = OllamaEmbeddings(model="nomic-embed-text", base_url=os.getenv("OLLAMA_URL"))
embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings, store, namespace=underlying_embeddings.model
)
embeddings = embedder.embed_documents(["hello", "goodbye"])
print(list(store.yield_keys()))


9.记忆
记忆的类型
1）会话内存
ConversationBufferMemory
2)滑动窗口内存
ConversationBufferWindowMemory
3)实体内存
ConversationEntityMemory
4)知识图谱会话内存
ConversationKGMemory
5)会话摘要内存
ConversationSummaryMemory
6)会话摘要缓存内存
ConversationSummaryBufferMemory
7)会话Token缓存内存
ConversationTokenBufferMemory
8)向量存储检索内存
VectorStoreRetrieverMemrory


11.工具
工具的关键要素。
1）名称：用以标识和引用。
2）描述：阐明工具的功能和用途。
3）JSON模式：定义了输入数据和结构和格式。
4）调用函数：实现具具体的功能。
5）标志：指示是否应直接将工具结果返回给用户。

内置工具：
Apify
Shell
Bing Search
File System
Memorize #微调LLM，以实现信息的记忆功能，需要支持微调的LLM
SQL Database

自定义工具：
1）使用@tool装饰器
@tool
def search(query: str) -> str:
    """在网上查东西"""
    return "LangChain"

@tool("search-tool", args_schema=SearchInput, return_direct=True)
def search(query: str) -> str:
    """在网上查东西。"""
    return "LangChain"

2）使用StructuredTool
from pydantic.v1 import BaseModel, Field
class CalculatorInput(BaseModel):
    a: int = Field(description="第一个数字")
    b: int = Field(description="第二个数字")


def multiply(a: int, b: int) -> int:
    """将两个数字相乘"""
    return a * b


calculator = StructuredTool.from_function(
    description="相乘",
    name="计算器",
    func=multiply,
    args_schema=CalculatorInput,
    return_direct=True
)

3)agent调用(没有怎么搞明白)

llm = ChatOllama(model=os.getenv("LLM_NAME"), base_url=os.getenv("OLLAMA_URL"))


def _should_check(serialized_obj: dict) -> bool:
    return serialized_obj.get("name") == "terminal"


def _approve(_input: str) -> bool:
    if _input == "echo 'Hello World'":
        return True

    msg = ("你是否批准以下输入？",
           "'Y'/'Yes'(不分大小写)之外的任务内容都将被视为'否'")
    msg += "\n\n" + _input + "\n"
    resp = input(msg)
    return resp.lower() in ("yes", "y")


callbacks = [HumanApprovalCallbackHandler(should_check=_should_check, approve=_approve)]
tools = load_tools(["terminal", "wikipedia", "llm-math"], llm=llm)
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

agent.run("现在是2024年，康拉德.阿登纳在哪一年成为德国总理？", callbacks=callbacks)



12.Agent
是一个智能决策单元，负责确定下一步的行动，其运行依赖语言模型和指令的支持。核心在于使用LLM来动态规划操作序列，这些操作可能涉及使用工具并观察其反馈，或赂用户发送相应的回复。
从操作序列固定的链相比，代理中的LLM作为推理引擎，负责智能决策所需执行的操作及顺序。
1）代理的输入
键值映射（intermediate_steps必需键）
PromptTemplate将这些键值对转换为适合的LLM处理的格式
可用工具列表
用户输入
执行过的任何中间步骤
2）代理的输出
接下来要执行的操作或发送给用户的最终响应。
AgintAction
AgintAction List
AgentFinish
输出解析器的任务是接收LLM的原始输出，并且将其转换为这3种类型之一。
3）代理执行器
运行时组件，实际调用代理并执行其选择的操作。
4）异步调用支持
5）代理执行迭代器 AgentExecutiveIterator


