1、lora
lora不需要调整所有的参数，仅需要更新一小部份低秩矩阵。


2、Llama-factory
2.1:https://github.com/hiyouga/LLaMA-Factory
2.2:bitsandbytes-0.39  #-0.48.3  bitsandbytes是对CUDA自定义函数的轻量级封装，特别是针对8位优化器、矩阵乘法（LLM.int8()）和量化函数。
2.3:tensorboard torch torchvision torchaudio
2.4: docker build -f ./Dockerfile -t llama-factory:latest .
docker run --gpus=all \
  -v ./hf_cache:/root/.cache/huggingface/ \
  -v ./data:/app/data \
  -v ./output:/app/output \
  -e CUDA_VISIBLE_DEVICES=0 \
  -p 7860:7860 \
  --shm-size 16G \
  --name llama_factory \
  -d llama-factory:latest

2.5 损失图意义：
2.5.1 收敛
收敛：损失值随着训练的进行不断降低，并趋近某个稳定值，则表明llm正在学习，并逼近最佳解。下降并逐渐平稳的曲线。
未收敛：如果波动很大，没有学习到，需要调整超参数或优化方法。
2.5.2 拟合
过拟合：损失值持续降低，验证损失值降到一定后开始上升，训练集上好，测试集上不行。
欠拟合：训练与验证损失值在较高水平，没有明显下降，模型复杂度不够。





3、微调涉及量化。   
使用Llama.cpp量化
3.1 sudo apt install cmake
3.2 git clone https://github.com/ggerganov/llama.cpp
3.3 new conda env  and activate env #name：llama_cpp
3.4 pip install -r requirements/requirements-convert-hf-to-gguf.txt
3.5 cmake -B build
3.6 cmake --build build --config Release

3.7 模型转换
python convert-hf-to-gguf.py /opt/modes/2024-1 --outtype f16 --outfile /opt/2024-2-2-llama3-zh.gguf
3.8 模型量化（q4_0）
应用q4_0量化方法。
./quantize /opt/2024-2-2-llama3-zh.gguf /opt/modes/2024-2-2-llama3-zh.gguf q4_0

4、部署模型（ollama）
4.1 Modelfile
/home/yematech/Modelfile
FROM /data/open-webui/models/2024-2-2-llama3-zh.gguf
EOF

4.2 create model
ollama create llama3-Chinese:8B -f Modelfile

4.3 run modle






