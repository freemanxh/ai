1、lora
lora不需要调整所有的参数，仅需要更新一小部份低秩矩阵。


2、Llama-factory
2.1:https://github.com/hiyouga/LLaMA-Factory
2.2:bitsandbytes-0.39  #-0.48.3  bitsandbytes是对CUDA自定义函数的轻量级封装，特别是针对8位优化器、矩阵乘法（LLM.int8()）和量化函数。
2.3:tensorboard torch torchvision torchaudio
2.4: docker build -f ./Dockerfile -t llama-factory:latest .
docker run --gpus=all \
  -v ./hf_cache:/root/.cache/huggingface/ \
  -v ./data:/app/data \
  -v ./output:/app/output \
  -e CUDA_VISIBLE_DEVICES=0 \
  -p 7860:7860 \
  --shm-size 16G \
  --name llama_factory \
  -d llama-factory:latest




3、微调涉及量化。   
使用Llama.cpp量化
3.1 sudo apt install cmake
3.2 git clone https://github.com/ggerganov/llama.cpp
3.3 new conda env  and activate env #name：llama_cpp
3.4 pip install -r requirements/requirements-convert-hf-to-gguf.txt
3.5 cmake -B build
3.6 cmake --build build --config Release

3.7 模型转换
python convert-hf-to-gguf.py /opt/modes/2024-1 --outtype f16 --outfile /opt/2024-2-2-llama3-zh.gguf
3.8 模型量化（q4_0）
应用q4_0量化方法。
./quantize /opt/2024-2-2-llama3-zh.gguf /opt/modes/2024-2-2-llama3-zh.gguf q4_0







