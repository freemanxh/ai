server:
1、安装
  docker pull chromadb/chroma
  docker run -d -v ./chroma_data:/chroma/chroma -p 8000:8000 -e IS_PERSISTENT=TRUE --name chromadb  chromadb/chroma:latest


client:
1、安装
  pip install chroma

  import chromadb
  client=chromadb.HttpClient(host="192.168.0.6",port=8000)

2、嵌入向量
  from chromadb.utils import embedding_functions
3、默认嵌入模型 all-MiniLM-L6-v2
  default_ef=embedding_functions.DefaultEmbeddingFunction()
4、把文本转化为向量
  val=default_ef("foo")

5、OpenAI的嵌入模型
  openai_ef=embedding_functions.OpenAIEmbeddingFunction(
    api_key="",
    model_name="text-embedding-ada-002"
  )

6、client与collection相关的api
client使用
1)heartbeat()
2)list_collections()
3)create_collection(name="",metadata={})
4)get_collection(name)
5)get_or_create_collection(name="",metadata={})
6)delete_collection(name) 
7)reset() #重置数据库，相当危险
8)get_version()
9)get_settings()

collection使用
1)count()
2)add(ids=["id1","id2","id3"],metadatas=[{"source1":"s1"},{"source2":"s2"},{"source3":"se"}],documents=["first","second","third"])
3)get() ids,where,limit,offset  #如果要看向量 加上include=['embeddings']
4)peek()
5)query()  query_embeddings,query_texts,n_results,
  where #根据元数据筛选,
    {
      "metadata_field": {
        "<Operator>": "<Value>"
      }
    }
    $eq $ne $gt $gte $lt $lte
  where_document #根据文档进行筛选
  $contains $and $or $in $nin
6)update() ids
7)upsert()
8)delete()
9)modify(name="new_name") 重命名集合


实战（与Ollama进行结合）：
1、切分文档
loader = TextLoader("./elon_musk.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

2、使用Ollama中的Embeddings模型 
ollamaEmbeddings = OllamaEmbeddings(model="nomic-embed-text", base_url=os.getenv("OLLAMA_URL"))
ollamaEmbeddings = OllamaEmbeddings(model="bge-m3:latest", base_url=os.getenv("OLLAMA_URL"))

ollamaEmbeddings.embed_query(str)
ollamaEmbeddings.embed_documents(list)

必须实现自定义的EmbeddingFunction的接口(以下两个接口都只支持数组，如果是文本，得改实现方法。)
class CustomEmbeddingFunction(EmbeddingFunction):
    def __call__(self, texts):
        embeddings = [ollamaEmbeddings.embed_query(x) for x in texts]
        return embeddings

class CustomEmbeddingFunction(EmbeddingFunction):
    def __call__(self, texts):
        return ollamaEmbeddings.embed_documents(texts)

cef = CustomEmbeddingFunction()

chroma_client = chromadb.HttpClient(host="192.168.0.6", port=8000)
coll = chroma_client.get_or_create_collection("test101", embedding_function=cef)
coll.add(ids=ids, metadatas=metadatas, documents=documents)

coll.get(ids=['id_1'],include=['embeddings']) #如果要看向量，必须include 'embeddings',但在实际的使用过程中，没有必要查看此项。





tools 的使用成功代码

llm = ChatOllama(model=os.getenv("LLM_NAME"), base_url=os.getenv("OLLAMA_URL"))


def multiplier(a: float, b: float) -> float:
    """将提供的浮点相加。"""
    return a + b


def zhaojia(condition: str) -> str:
    """供造价问题调用"""
    return "hahaha调用我了"+condition+"：一立方米为1000元"

def parsing_multiplier(string: str):
    a, b = string.split(",")
    return multiplier(int(a),int(b))


tool = StructuredTool.from_function(multiplier)
tool2 = StructuredTool.from_function(zhaojia)

# 结构化工具与Structured_CHAT_ZERO_SHOT_RACT_DESCRIPTION代理类型兼容。
agent_executor = initialize_agent(
    [tool, tool2,Tool(name="parsing_Multiplier", func=parsing_multiplier,
                       description="该工具的输入应该是一个以逗号分隔的长度为2的数字列表，表示要相加的两个数字。例如，如果要将1+2，则输入`1,2`。"],
    llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)

# agent_executor.invoke("2 + 7是多少？")
agent_executor.invoke("这个材料值多少钱，混凝土一立方米?")
















