https://llamafactory.readthedocs.io/zh-cn/latest/index.html


指令监督微调 Supervised finetuning (SFT)
奖励模型训练 Reward model Training (RLHF)
LoRA Low-Rank Adaptation，LoRA 参数高效微调方法，  低秩


1.1.2 运算精度
1.1.2.1 16比特（半精度浮点数）全参数微调：指的是在微调过程中对模型的所有参数进行训练，并使用16比特的精度（通常是FP16，即半精度浮点数）来存储和计算模型权重。
全参数微调（Full Parameter Fine-tuning）：
  ○ 全参数微调指的是对模型的所有参数进行更新。与此相对的是参数高效微调方法，只调整部分参数，保持其他参数冻结。
  ○ 在全参数微调中，模型的所有权重都在微调数据上进行学习调整，以适应新的任务或数据分布。
16比特精度（FP16）：
  ○ 16比特精度表示模型权重和梯度在计算时采用16位浮点数表示，取代传统的32位浮点数。
  ○ 16比特浮点数需要的存储空间更小，计算速度也更快，因此可以显著节省显存和加快计算速度。
  ○ FP16在训练过程中能有效减少内存占用，适合处理大型模型的全参数微调。
混合精度训练：
  16比特全参数微调通常采用混合精度训练（Mixed Precision Training），即在计算过程中使用16位浮点数表示大部分参数和操作，但在一些关键计算（如反向传播中的梯度累加）上使用32位浮点数，以保证稳定性和精度。

1.1.2.2 冻结微调：冻结微调（Freezing Fine-tuning）是一种参数高效微调方法，它通过在模型微调时冻结部分参数，只对其他部分参数进行训练，以达到减少计算资源和显存消耗的目的。
分为“冻结层”和“可训练层”
1. 冻结层：
  ○ 冻结层指的是在微调过程中不更新的参数。通常是模型中低层次的部分（如Transformer模型的前几层或预训练模型的卷积层等），因为这些层已经学习了足够多的通用特征。
  ○ 冻结这些层可以大幅减少计算开销，因为这些参数不需要计算梯度，从而节省显存和加快训练。
2. 可训练层：
  ○ 可训练层是模型中需要更新的部分。通常是模型的高层次（接近输出层）的部分，因为这些层包含了与任务更直接相关的特征。
  ○ 只训练这些层可以让模型更好地适应新的任务或领域，而不会大幅增加资源需求。
冻结微调的优点
1. 降低资源消耗：冻结一部分层后，计算资源和显存需求大幅降低，使得在显存较小的硬件上也能微调大型模型。
2. 加快训练速度：由于部分参数不更新，训练过程中的梯度计算减少，从而加速了微调过程。
3. 避免过拟合：只微调高层参数可以减少过度拟合的风险，特别是在小规模数据集上进行微调时。
冻结微调的常见策略
1. 部分冻结：只训练模型的后几层或输出层，冻结其余部分。例如，冻结BERT模型的前几层，微调最后几层使模型适应特定的下游任务。
2. 逐步解冻：从冻结状态开始，逐渐解冻更多层。这种方式可以让模型从高层特征逐渐过渡到低层特征的微调，在某些任务中效果较好。
3. 任务特定解冻：在特定的任务需求下选择性解冻相关层。例如，对于情感分类任务，可能只需要解冻语言模型中的高层次层数，以提取情感相关的特征。

1.1.2.3 LoRA 微调：LoRA 微调（Low-Rank Adaptation, LoRA）是一种参数高效微调方法，通过在神经网络中引入低秩矩阵分解来大幅减少参数更新量，以实现低计算开销的微调。LoRA 主要应用于大语言模型（如GPT、BERT等）和其他深度神经网络模型的微调，使其能够在资源有限的环境中高效适应新任务。
LoRA 通过以下两方面实现高效微调：
1. 低秩矩阵分解：
  ○ LoRA 将模型中的权重矩阵表示为一个基础权重加上一个低秩矩阵分解。低秩矩阵通常包含两个小矩阵（A和B），用来近似模型参数的变化。这种方式大幅减少了需要训练的参数数量。
  ○ 在微调过程中，模型的原始参数保持不变，只训练这些小的低秩矩阵参数，这使得训练过程更高效。
2. 模块化微调：
  ○ LoRA 将新增的低秩矩阵作为一个可插拔的模块，嵌入在模型的特定层中，而不改变模型的原始架构。这种模块化方法可以将新增参数独立存储，方便后续快速切换不同任务所需的低秩适应参数。
  ○ 在应用任务时，模型将原始权重和低秩矩阵的输出相加，得到最终输出。
LoRA 微调的优点
1. 减少计算资源需求：只训练少量的低秩矩阵，大幅减少了显存和计算需求，相比于全参数微调更加高效。
2. 避免过拟合：LoRA 只更新少数低秩参数，有助于保持模型的原始特性，从而降低过拟合风险。
3. 模块化、任务适应性强：LoRA 的低秩矩阵是独立存储的，可以为每个任务存储特定的低秩参数，便于不同任务间的快速切换。
4. 可拓展性：LoRA 可以与其他参数高效微调方法（如冻结微调、指令微调）结合，以实现更好的微调效果。

LoRA 的工作流程通常包含以下步骤：
1. 矩阵分解：选择需要微调的模型层，将其权重矩阵分解成低秩矩阵。
2. 低秩更新：在微调过程中，只更新低秩矩阵的参数，而冻结其他模型权重。
3. 结果组合：推理时，计算模型的输出为原始权重输出加上低秩矩阵的输出，从而得到适应新任务的结果。

1.1.2.4 QLoRA 微调：QLoRA（Quantized Low-Rank Adapter）是一种用于微调大型语言模型（LLM）的技术。它结合了低秩适配器（LoRA）和量化技术，旨在减少模型在微调过程中的内存占用和计算成本，同时尽量保持模型性能。
1. LoRA：引入一个低秩矩阵来近似原始模型中的高维权重矩阵。只更新一小部分参数，从而减少了微调过程中的内存和计算需求。
2. 量化（Quantization）：QLoRA进一步采用了量化技术，将模型的权重从浮点数（如32位或16位）转换为更低精度的表示（如4位）。量化可以显著减少模型的内存占用，但同时也可能引入一些精度损失。
3. NormalFloat量化：QLoRA使用了一种称为NormalFloat的量化技术，它特别适合于正态分布的数据。这种量化方法在信息理论上是最优的，可以产生比传统的整数或浮点量化更好的结果。
4. 双重量化（Double Quantization）：QLoRA还采用了双重量化技术，对量化常数本身进行量化，从而进一步减少内存使用。
5. 分页优化器（Paging Optimizer）：为了处理长序列长度的小批量数据，QLoRA使用了分页优化器来管理内存峰值。这种优化器利用NVIDIA统一内存技术，在CPU和GPU之间自动进行数据传输，以避免内存溢出。


1.1.2.5 量化技术：量化技术是指在深度学习中，将模型参数和计算精度从高位（如32位浮点数，FP32）转换为低位（如16位浮点数、8位或4位整数等）的过程。这种转换能够显著降低模型的计算和存储成本，从而提高推理速度、减少内存占用，并使得在资源受限的硬件（如手机、边缘设备）上运行大型模型成为可能。
量化技术的核心概念
1. 数值精度降低：
  ○ 传统的神经网络模型通常使用32位浮点数（FP32）来存储权重、激活值和计算结果。量化技术通过将FP32精度降低到16位（FP16）或更低位数（如8位整数、4位整数等），减少了计算复杂度和存储需求。
2. 离散化过程：
  ○ 量化是一种离散化过程，将模型参数从连续的浮点数空间映射到较小的离散值集合。例如，将一个32位浮点数映射到0-255范围的8位整数集合，这样的映射可以极大地减少模型的内存占用和计算量。

量化技术的优点
1. 减少内存占用：量化后模型权重和激活值精度降低，占用的存储空间变小，适合在存储受限的设备上运行。
2. 加快推理速度：较低精度的计算能加速模型推理，特别适合在边缘设备和移动设备上实时推理的场景。
3. 节省能源：计算复杂度降低，使得量化模型的能耗更低，有利于节能。


1.1.3 加速算子
1.1.3.1 FlashAttention-2
FlashAttention2是一种针对GPU优化的自注意力机制算法，旨在提高计算效率并减少内存使用。它是FlashAttention的改进版本，通过一系列工程优化，使得在GPU上的计算性能得到显著提升

1.1.3.2 Unsloth
Unsloth是一个开源的大语言模型（LLM）微调加速工具，它能够显著提高微调的速度和效率，同时大幅降低内存占用。

1.1.4 推理引擎
1.1.4.1 Transformers （modelscope）

from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello world!", return_tensors="pt")
outputs = model(**inputs)

1.1.4.2 vLLM
vLLM是一个专为大规模语言模型（LLM）推理优化的服务框架和推理引擎。它通过一系列技术创新，显著提高了推理效率，降低了内存占用，特别适合处理需要高吞吐量和低延迟的NLP任务。
from vllm import LLM, SamplingParams

prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

llm = LLM(model="facebook/opt-125m")

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")




1.1.5 实验面板
1.1.5.1 LlamaBoard
LlamaBoard 的主要特点包括：
1. 轻松配置：用户可以通过与Web界面的交互来自定义微调参数，并为大多数参数提供默认值，简化了配置过程。
2. 可监控的训练：训练过程中，训练日志和损失曲线被可视化并实时更新，使用户能够监控训练进度。
3. 灵活评估：支持计算数据集上的文本相似度分数以自动评估模型，或通过与模型聊天进行人工评估。
4. 多语言支持：提供本地化文件，方便集成新语言以渲染界面。目前支持英语、俄语和中文，允许更广泛的用户使用LlamaBoard进行大模型的微调 。

1.1.5.2 TensorBoard
TensorBoard的主要功能包括：
1. 损失和准确度曲线：TensorBoard可以绘制训练和验证过程中的损失和准确度等关键指标的曲线图，帮助用户监控模型的学习进度。
2. 权重和激活直方图：它可以显示模型中权重和激活的分布情况，这有助于识别模型是否出现了梯度消失或梯度爆炸等问题。
3. 高维数据可视化：TensorBoard支持t-SNE和PCA等降维技术，可以将高维数据投影到二维或三维空间中进行可视化。
4. 图像和视频可视化：在处理图像或视频数据时，TensorBoard可以显示批次中的图像或帧，以及模型生成的图像。
5. 注意力机制可视化：对于使用注意力机制的模型，如Transformer，TensorBoard可以可视化注意力权重，帮助理解模型是如何关注输入序列的不同部分的。
6. 模型图：TensorBoard可以显示模型的计算图，包括层、操作和张量，这有助于理解模型的结构和数据流向。
7. 自定义仪表板：用户可以创建自定义的仪表板来展示特定的信息或指标。

1.1.5.3 Wandb
Wandb（Weights & Biases）是一个强大的机器学习实验跟踪和可视化工具，它提供了一种简便的方法来记录和监控模型训练过程中的各种指标，包括超参数、训练和验证损失、准确率等。
1. 实验跟踪和版本控制：Wandb可以记录和跟踪机器学习实验，包括超参数、指标、模型架构等，帮助复现模型和进行版本管理 。
2. 可视化和分析：提供丰富的可视化工具，可以直观地展示实验结果、训练曲线、指标趋势等，支持创建交互式图表、散点图、直方图等 。
3. 模型登记和部署：帮助登记和管理训练的模型，包括模型文件、权重和元数据，便于共享和部署模型。
4. 协作和共享：提供团队协作功能，可以邀请团队成员参与实验、查看结果，并进行讨论和反馈。
5. 集成和兼容性：与常用的机器学习框架（如PyTorch、TensorFlow、Scikit-learn等）以及其他工具（如Jupyter Notebook、Docker等）具有良好的集成和兼容性 。
6. 数据和模型管理：提供数据和模型备份管理功能，支持数据集和模型版本化 。
7. 超参数搜索：Wandb提供了超参数搜索功能，帮助优化模型性能 。
8. 本地部署：Wandb还提供本地服务器部署功能，允许在没有互联网连接的情况下使用 。

1.1.5.4 MLflow
MLflow是一个开源的机器学习生命周期管理平台，它提供了一套工具来简化机器学习工作流程。MLflow的主要目标是帮助数据科学家和ML工程师在整个机器学习项目的开发、部署和管理过程中提高效率和一致性。
1. MLflow Tracking：这是MLflow的一个核心组件，提供了API和用户界面，用于记录机器学习过程中的参数、代码版本、指标和输出文件。这使得团队可以追踪模型的演变，并比较不同运行的结果 。
2. MLflow Projects：MLflow Projects允许用户将机器学习代码打包成可重用和可重现的格式，类似于可执行程序。这使得其他用户可以轻松地再次运行这些项目，并可靠地重现结果 。
3. MLflow Models：MLflow提供了一种简单的模型封装格式，允许用户将模型部署到多种模型服务和推理平台。例如，可以将模型封装为Python函数，并将其部署到Docker或Azure ML进行线上服务 。
4. MLflow Model Registry：这是一个集中式模型存储库，提供了API和用户界面，用于协作管理MLflow模型的完整生命周期。它包括模型版本控制、别名、标签和注释，有助于处理不同版本的模型，并确保它们的顺利生产化 。
5. 实验管理：允许用户记录和比较不同实验中的模型性能、参数和指标 。
6. 部署一致性：提供了标准化的方式来打包和部署模型，确保了从开发到生产的一致性 。


所需环境：
pytorch 2.1.0 
python 3.10 ubuntu 22.04
cuda 12.1

3080x2 or 3090x1
mem：48G

安装过程
conda create -n llama_factory python=3.10
conda activate llama_factory

git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
#git clone --depth 1 https://bgithub.xyz/hiyouga/LLaMA-Factory.git

cd LLaMA-Factory
pip install -e ".[torch,metrics]"
-e：以”开发模式“安装，对代码的任何更改都会立即反映在安装的包中，不需要重新安装。
[torch,metrics]：对包的特定依赖进行安装，指定两个额外的依赖：torch和metrice。

用llmafactory-cli version进行校验安装是否成功。

配置huggingface api 镜像
export HF_HOME=/root/autodl-tmp/huggingface-cache/
export HF_ENDPOINT=https://hf-mirror.com

下载模型
pip install modelscope
modelscope download --model=LLM-Research/Meta-Llama-3.1-8b-Instruct --local_dir Meta-Llama-3.1-8B-Instruct


2.1 修改文件
2.1.1 train_lora/llama3_lora_sft.yaml
文件位置：./LLaMA-Factory/examples/train_lora/

### model
model_name_or_path: /root/autodl-tmp/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct #模型名称或路径

### method
stage: sft #微调的方法（可选: rm(reward modeling), pt(pretrain), sft(Supervised Fine-Tuning), PPO, DPO, KTO, ORPO），这里我们是有监督指令微调，所以是sft
do_train: true #是否进行训练 (true用于训练, false用于评估)
finetuning_type: lora #使用lora技术 (可选: freeze, lora, full)
lora_target: all #目标模型中的所有参数 (默认值为 all)

### dataset #训练数据集
dataset: identity,alpaca_en_demo #数据集的名称 (使用”,”分隔多个数据集)
template: llama3 #使用的模板，不同模型不同，请参考 https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#supported-models，获取不同模型的模板定义，否则会回答结果会很奇怪或导致重复生成等现象的出现。chat 版本的模型基本都需要指定
cutoff_len: 1024 #输入的最大长度
max_samples: 1000 #最大样本数
overwrite_cache: true 
preprocessing_num_workers: 16 #使用训练线程的数量

### output #输入目录和其他输出相关的设置
output_dir: saves/llama3.1-8b-instruct/lora/sft
logging_steps: 10 #日志记录的步数
save_steps: 500 #保存模型的步数
plot_loss: true #是否绘制损失图
overwrite_output_dir: true #是否覆盖输出目录

### train #训练过程中的设置
per_device_train_batch_size: 1 #每个设备上的批处理大小，如果GPU显存够大，可以适当增加。
gradient_accumulation_steps: 8 #梯度累积的步数
#max_grad_norm: x #梯度裁剪阈值
learning_rate: 1.0e-4 #学习率
num_train_epochs: 3.0 #训练的轮数
lr_scheduler_type: cosine #学习率调度器的类型，也叫学习率曲线 （可选 linear, cosine, polynomial, constant 等）
#num_train_epochs: #训练周期数
warmup_ratio: 0.1 #学习率预热比例
#warmup_steps: x #学习率预热步数
bf16: true #是否使用bf16精度（半精度）
ddp_timeout: 180000000 #分布式数据并行（DDP）的超时时间


### eval #评估的设置
val_size: 0.1 #验证集的大小
per_device_eval_batch_size: 1 #每个设备上的评估批次大小
eval_strategy: steps #评估策略
eval_steps: 500 #评估的步数


model：指定了模型的路径和名称
method:微调的方法 
stage: sft #微调的方法
do_train: true 


2.1.2 推理使用配置
inference/llama3_lora_sft.yaml

model_name_or_path: /root/autodl-tmp/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct #原始模型
adapter_name_or_path: saves/llama3.1-8b-instruct/lora/sft #微调后的数据
template: llama3
finetuning_type: lora

2.1.3 合并模型与微调数据
### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: /root/autodl-tmp/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct
adapter_name_or_path: saves/llama3.1-8b-instruct/lora/sft
template: llama3
finetuning_type: lora

### export
export_dir: models/llama3.1_8b_instruct_lora_sft
export_size: 2
export_device: cpu
export_legacy_format: false

2.1.4 merge_lora/llama3_gptq.yaml
选择通过量化技术对模型进行压缩，从而实现更高效的部署。
GPTQ 等后训练量化方法(Post Training Quantization)是一种在训练后对预训练模型进行量化的方法。通过量化技术将高精度表示的预训练模型转换为低精度的模型，从而在避免过多损失模型性能的情况下减少显存占用并加速推理，希望低精度数据类型在有限的表示范围内尽可能地接近高精度数据类型的表示，因此需要指定量化位数 export_quantization_bit 以及校准数据集 export_quantization_dataset。
### examples/merge_lora/llama3_gptq.yaml
### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
template: llama3


### export
export_dir: models/llama3_gptq
export_quantization_bit: 4  # 量化位数
export_quantization_dataset: data/c4_demo.json  #量化校准数据集
export_size: 2 #最大导出模型文件大小
export_device: cpu #导出设备
export_legacy_format: false #是否使用旧格式导出



2.2.1 微调
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml

2.2.2 推理
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
训练完后可以在设置的output_dir下看到如下内容，主要包含3部分
1. adapter开头的就是 LoRA保存的结果了，后续用于模型推理融合
2. training_loss 和trainer_log等记录了训练的过程指标
3. 其他是训练当时各种参数的备份

2.2.3 合并
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml

2.3 量化
llamafactory-cli export examples/merge_lora/llama3_gptq.yaml

2.4 评估
2.4.1 通用能力评估
lamafactory-cli eval examples/train_lora/llama3_lora_eval.yaml

### examples/train_lora/llama3_lora_eval.yaml
### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
adapter_name_or_path: saves/llama3-8b/lora/sft # 可选项

### method
finetuning_type: lora

### dataset
task: mmlu_test #评估任务的名称，可选项有 mmlu_test, ceval_validation, cmmlu_test
template: fewshot
lang: en
n_shot: 5

### output
save_dir: saves/llama3-8b/lora/eval

### eval
batch_size: 4 #每个GPU使用的批量大小，默认值为 4。


2.4.2 NLG 评估
llamafactory-cli train examples/extras/nlg_eval/llama3_lora_predict.yaml

### examples/extras/nlg_eval/llama3_lora_predict.yaml
### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
adapter_name_or_path: saves/llama3-8b/lora/sft

### method
stage: sft
do_predict: true
finetuning_type: lora

### dataset
eval_dataset: identity,alpaca_en_demo
template: llama3
cutoff_len: 2048
max_samples: 50
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: saves/llama3-8b/lora/predict
overwrite_output_dir: true

### eval
per_device_eval_batch_size: 1
predict_with_generate: true
ddp_timeout: 180000000


还可以通过指令
python scripts/vllm_infer.py --model_name_or_path path_to_merged_model --dataset alpaca_en_demo中指定模型、数据集以使用 vllm 推理框架以取得更快的推理速度。




2.6 LLaMA Board（WebUI）可视化微调
设置gradio 共享
export GRADIO_SHARE=true
启动LLaMA Board
llamafactory-cli webui






