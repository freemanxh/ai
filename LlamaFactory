https://llamafactory.readthedocs.io/zh-cn/latest/index.html


指令监督微调 Supervised finetuning (SFT)
奖励模型训练 Reward model Training (RLHF)
LoRA Low-Rank Adaptation，LoRA 参数高效微调方法，  低秩

所需环境：
pytorch 2.1.0 
python 3.10 ubuntu 22.04
cuda 12.1

3080x2 or 3090x1
mem：48G

安装过程
conda create -n llama_factory python=3.10
conda activate llama_factory

git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
#git clone --depth 1 https://bgithub.xyz/hiyouga/LLaMA-Factory.git

cd LLaMA-Factory
pip install -e ".[torch,metrics]"
-e：以”开发模式“安装，对代码的任何更改都会立即反映在安装的包中，不需要重新安装。
[torch,metrics]：对包的特定依赖进行安装，指定两个额外的依赖：torch和metrice。

用llmafactory-cli version进行校验安装是否成功。

配置huggingface api 镜像
export HF_HOME=/root/autodl-tmp/huggingface-cache/
export HF_ENDPOINT=https://hf-mirror.com

下载模型
pip install modelscope
modelscope download --model=LLM-Research/Meta-Llama-3.1-8b-Instruct --local_dir Meta-Llama-3.1-8B-Instruct


2.1 修改文件
2.1.1 train_lora/llama3_lora_sft.yaml
文件位置：./LLaMA-Factory/examples/train_lora/

### model
model_name_or_path: /root/autodl-tmp/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct #模型名称或路径

### method
stage: sft #微调的方法（可选: rm(reward modeling), pt(pretrain), sft(Supervised Fine-Tuning), PPO, DPO, KTO, ORPO），这里我们是有监督指令微调，所以是sft
do_train: true #是否进行训练 (true用于训练, false用于评估)
finetuning_type: lora #使用lora技术 (可选: freeze, lora, full)
lora_target: all #目标模型中的所有参数 (默认值为 all)

### dataset #训练数据集
dataset: identity,alpaca_en_demo #数据集的名称 (使用”,”分隔多个数据集)
template: llama3 #使用的模板，不同模型不同，请参考 https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#supported-models，获取不同模型的模板定义，否则会回答结果会很奇怪或导致重复生成等现象的出现。chat 版本的模型基本都需要指定
cutoff_len: 1024 #输入的最大长度
max_samples: 1000 #最大样本数
overwrite_cache: true 
preprocessing_num_workers: 16 #使用训练线程的数量

### output #输入目录和其他输出相关的设置
output_dir: saves/llama3.1-8b-instruct/lora/sft
logging_steps: 10 #日志记录的步数
save_steps: 500 #保存模型的步数
plot_loss: true #是否绘制损失图
overwrite_output_dir: true #是否覆盖输出目录

### train #训练过程中的设置
per_device_train_batch_size: 1 #每个设备上的批处理大小，如果GPU显存够大，可以适当增加。
gradient_accumulation_steps: 8 #梯度累积的步数
#max_grad_norm: x #梯度裁剪阈值
learning_rate: 1.0e-4 #学习率
num_train_epochs: 3.0 #训练的轮数
lr_scheduler_type: cosine #学习率调度器的类型，也叫学习率曲线 （可选 linear, cosine, polynomial, constant 等）
#num_train_epochs: #训练周期数
warmup_ratio: 0.1 #学习率预热比例
#warmup_steps: x #学习率预热步数
bf16: true #是否使用bf16精度（半精度）
ddp_timeout: 180000000 #分布式数据并行（DDP）的超时时间


### eval #评估的设置
val_size: 0.1 #验证集的大小
per_device_eval_batch_size: 1 #每个设备上的评估批次大小
eval_strategy: steps #评估策略
eval_steps: 500 #评估的步数


model：指定了模型的路径和名称
method:微调的方法 
stage: sft #微调的方法
do_train: true 


2.1.2 推理使用配置
inference/llama3_lora_sft.yaml

model_name_or_path: /root/autodl-tmp/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct #原始模型
adapter_name_or_path: saves/llama3.1-8b-instruct/lora/sft #微调后的数据
template: llama3
finetuning_type: lora

2.1.3 合并模型与微调数据
### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: /root/autodl-tmp/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct
adapter_name_or_path: saves/llama3.1-8b-instruct/lora/sft
template: llama3
finetuning_type: lora

### export
export_dir: models/llama3.1_8b_instruct_lora_sft
export_size: 2
export_device: cpu
export_legacy_format: false

2.1.4 merge_lora/llama3_gptq.yaml
选择通过量化技术对模型进行压缩，从而实现更高效的部署。
GPTQ 等后训练量化方法(Post Training Quantization)是一种在训练后对预训练模型进行量化的方法。通过量化技术将高精度表示的预训练模型转换为低精度的模型，从而在避免过多损失模型性能的情况下减少显存占用并加速推理，希望低精度数据类型在有限的表示范围内尽可能地接近高精度数据类型的表示，因此需要指定量化位数 export_quantization_bit 以及校准数据集 export_quantization_dataset。
### examples/merge_lora/llama3_gptq.yaml
### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
template: llama3


### export
export_dir: models/llama3_gptq
export_quantization_bit: 4  # 量化位数
export_quantization_dataset: data/c4_demo.json  #量化校准数据集
export_size: 2 #最大导出模型文件大小
export_device: cpu #导出设备
export_legacy_format: false #是否使用旧格式导出



2.2.1 微调
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml

2.2.2 推理
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
训练完后可以在设置的output_dir下看到如下内容，主要包含3部分
1. adapter开头的就是 LoRA保存的结果了，后续用于模型推理融合
2. training_loss 和trainer_log等记录了训练的过程指标
3. 其他是训练当时各种参数的备份

2.2.3 合并
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml

2.3 量化
llamafactory-cli export examples/merge_lora/llama3_gptq.yaml

2.4 评估
2.4.1 通用能力评估
lamafactory-cli eval examples/train_lora/llama3_lora_eval.yaml

### examples/train_lora/llama3_lora_eval.yaml
### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
adapter_name_or_path: saves/llama3-8b/lora/sft # 可选项

### method
finetuning_type: lora

### dataset
task: mmlu_test #评估任务的名称，可选项有 mmlu_test, ceval_validation, cmmlu_test
template: fewshot
lang: en
n_shot: 5

### output
save_dir: saves/llama3-8b/lora/eval

### eval
batch_size: 4 #每个GPU使用的批量大小，默认值为 4。


2.4.2 NLG 评估
llamafactory-cli train examples/extras/nlg_eval/llama3_lora_predict.yaml

### examples/extras/nlg_eval/llama3_lora_predict.yaml
### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
adapter_name_or_path: saves/llama3-8b/lora/sft

### method
stage: sft
do_predict: true
finetuning_type: lora

### dataset
eval_dataset: identity,alpaca_en_demo
template: llama3
cutoff_len: 2048
max_samples: 50
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: saves/llama3-8b/lora/predict
overwrite_output_dir: true

### eval
per_device_eval_batch_size: 1
predict_with_generate: true
ddp_timeout: 180000000


还可以通过指令
python scripts/vllm_infer.py --model_name_or_path path_to_merged_model --dataset alpaca_en_demo中指定模型、数据集以使用 vllm 推理框架以取得更快的推理速度。




2.6 LLaMA Board（WebUI）可视化微调
设置gradio 共享
export GRADIO_SHARE=true
启动LLaMA Board
llamafactory-cli webui






